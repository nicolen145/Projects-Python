{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea933ca9-1532-4406-b540-e6926a62824d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.rdd import RDD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac689458-c1b8-4aba-a47f-feabc3bcb8db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def Kmeans(data, k, ct=0.0001, iterations=30, initial_centroids=None):\n",
    "    \"\"\"\n",
    "    Implements the K-means clustering algorithm with robust handling of missing clusters.\n",
    "    \"\"\"\n",
    "    # Step 1: Prepare and normalize the data\n",
    "    data = PrepareData(data)\n",
    "    data_rdd = data.rdd.map(lambda row: row[\"scaled_features\"].toArray())\n",
    "\n",
    "    # Step 2: Initialize centroids\n",
    "    if initial_centroids:\n",
    "        centroids = np.array(initial_centroids, dtype=float)\n",
    "    else:\n",
    "        centroids = np.array(data_rdd.takeSample(False, k), dtype=float)\n",
    "    \n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        # Assign each point to the nearest centroid\n",
    "        def assign_cluster(point):\n",
    "            distances = [np.linalg.norm(point - c) for c in centroids]\n",
    "            cluster_id = np.argmin(distances)\n",
    "            return (cluster_id, point)\n",
    "\n",
    "        clustered_rdd = data_rdd.map(assign_cluster)\n",
    "\n",
    "        # Count points in each cluster\n",
    "        cluster_counts = clustered_rdd.map(lambda x: x[0]).countByValue()\n",
    "        \n",
    "\n",
    "        # Handle missing clusters\n",
    "        missing_clusters = set(range(k)) - set(cluster_counts.keys())\n",
    "        if missing_clusters:\n",
    "            # Reinitialize missing clusters with random points\n",
    "            reinitialized_centroids = np.array(data_rdd.takeSample(False, len(missing_clusters)), dtype=float)\n",
    "            for i, missing_cluster in enumerate(missing_clusters):\n",
    "                centroids[missing_cluster] = reinitialized_centroids[i]\n",
    "            \n",
    "\n",
    "        # Recalculate centroids\n",
    "        def sum_points(p1, p2):\n",
    "            return [x + y for x, y in zip(p1, p2)]\n",
    "\n",
    "        cluster_sum_rdd = clustered_rdd.mapValues(lambda x: (x, 1)) \\\n",
    "                                       .reduceByKey(lambda a, b: (sum_points(a[0], b[0]), a[1] + b[1]))\n",
    "        new_centroids_rdd = cluster_sum_rdd.mapValues(lambda x: [v / x[1] for v in x[0]])\n",
    "        new_centroids = new_centroids_rdd.map(lambda x: x[1]).collect()\n",
    "\n",
    "        # Check for convergence\n",
    "        centroid_shift = sum(np.linalg.norm(np.array(new_c) - np.array(old_c)) for new_c, old_c in zip(new_centroids, centroids))\n",
    "\n",
    "        if centroid_shift < ct:\n",
    "            break\n",
    "\n",
    "        centroids = np.array(new_centroids, dtype=float)\n",
    "\n",
    "\n",
    "    return [tuple(map(lambda x: round(x, 5), c)) for c in centroids]\n",
    "\n",
    "\n",
    "def PrepareData(data):\n",
    "    \"\"\"\n",
    "    Prepares the data for clustering by normalizing features.\n",
    "    \"\"\"\n",
    "    feature_columns = data.columns[:-1]\n",
    "    vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    data_features = vector_assembler.transform(data).select(\"features\")\n",
    "\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "    scaler_model = scaler.fit(data_features)\n",
    "    data_normalized = scaler_model.transform(data_features).select(\"scaled_features\")\n",
    "    return data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ca142aa-d60f-4c8b-ba22-f72b9023b0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \"\"\" ******************* Importing the data ******************* \"\"\"\n",
    "\n",
    "# Iris = \"/FileStore/tables/Iris.csv\"\n",
    "# file_type = \"csv\"\n",
    "# infer_schema = \"True\"\n",
    "# first_row_is_header = \"True\"\n",
    "# delimiter = \",\"\n",
    "# iris = spark.read.format(file_type) \\\n",
    "#     .option(\"inferSchema\", infer_schema) \\\n",
    "#     .option(\"header\", first_row_is_header) \\\n",
    "#     .option(\"sep\", delimiter) \\\n",
    "#     .load(Iris)\n",
    "\n",
    "# # \"\"\" ******************* Example for testing ******************* \"\"\"\n",
    "# new_centroids = Kmeans(iris, 2, initial_centroids=[(0.5,0.5,0.5,0.5,0.5),(0.3,0.3,0.3,0.3,0.3)])\n",
    "# round_new_centroids=[tuple(round(num, 5) for num in tup) for tup in new_centroids]\n",
    "# expected_new_centroids=[(0.16871,0.19553,0.58252,0.08475,0.06618),(0.67067,0.54882,0.36532,0.66478,0.65951)]\n",
    "# if (not len(new_centroids)==len(expected_new_centroids)):\n",
    "#     print(\"Failed - Number of clusters is different than requested\")\n",
    "# if set(round_new_centroids)==set(expected_new_centroids):\n",
    "#     print(\"The test passed successfully\")\n",
    "# else:\n",
    "#     print(\"The test failed\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hw4_ 209296169_209470715_318504222",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
